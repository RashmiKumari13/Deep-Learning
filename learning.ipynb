{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c6574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_size=2\n",
    "hidden_size=10\n",
    "lr = 0.1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f86cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "  return np.maximum(0, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f03f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative( z):\n",
    "  return (z > 0).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80983ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65131593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative( a):\n",
    "    return sigmoid(a) * (1 - sigmoid(a))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e28351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Forward Pass -----------------------------------------------------------------\n",
    "def forward(X,V,W,b1, b2):\n",
    "  Z1 = X @ V + b1\n",
    "  A1 = relu(Z1)\n",
    "  Z2 = A1 @ W + b2\n",
    "  A2 = sigmoid(Z2)\n",
    "  return A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df22d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error Loss ----------\n",
    "def compute_loss(y, y_hat):\n",
    "  return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0ced069",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ---------- -------------------------------------Backpropagation ----------\n",
    "def backward(X,V,W,b1, b2, y):\n",
    "  m = X.shape[0]\n",
    "  Z1 = X @ V + b1\n",
    "  A1 = relu(Z1)\n",
    "  Z2 = A1 @ W + b2\n",
    "  A2 = sigmoid(Z2)\n",
    "  # Output layer gradient (MSE + Sigmoid)\n",
    "  dA2 = 2 * (A2 - y)\n",
    "  dZ2 = dA2 * sigmoid_derivative(A2)\n",
    "  dW = A1.T @ dZ2 / m\n",
    "  db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "  # Hidden layer gradient\n",
    "  dA1 = dZ2 @ W.T\n",
    "  dZ1 = dA1 * relu_derivative(Z1)\n",
    "\n",
    "  dV = X.T @ dZ1 / m\n",
    "  db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Parameter updates\n",
    "  W -= lr * dW\n",
    "  b2 -= lr * db2\n",
    "  V -= lr * dV\n",
    "  b1 -= lr * db1\n",
    "\n",
    "    # ---------- Training ----------\n",
    "def fit( X,V,W,b1, b2, y, epochs=100):\n",
    "  for epoch in range(epochs):\n",
    "    y_hat = forward(X,V,W,b1, b2)\n",
    "    loss = compute_loss(y, y_hat)\n",
    "    backward(X,V,W,b1, b2, y)\n",
    "    if epoch % 10 == 0:\n",
    "      print(f\"Epoch {epoch}, MSE Loss: {loss:.6f}\")\n",
    "   # ---------- Prediction ----------\n",
    "def predict(X,V,W,b1, b2, y, threshold=0.5):\n",
    "  probs = forward(X,V,W,b1, b2)\n",
    "  return (probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44607bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training ----------\n",
    "def fit( X,V,W,b1, b2, y, epochs=100):\n",
    "  for epoch in range(epochs):\n",
    "    y_hat = forward(X,V,W,b1, b2)\n",
    "    loss = compute_loss(y, y_hat)\n",
    "    backward(X,V,W,b1, b2, y)\n",
    "    if epoch % 10 == 0:\n",
    "      print(f\"Epoch {epoch}, MSE Loss: {loss:.6f}\")\n",
    "   # ---------- Prediction ----------\n",
    "def predict(X,V,W,b1, b2, y, threshold=0.5):\n",
    "  probs = forward(X,V,W,b1, b2)\n",
    "  return (probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2e0f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, MSE Loss: 0.233776\n",
      "Epoch 10, MSE Loss: 0.202810\n",
      "Epoch 20, MSE Loss: 0.189516\n",
      "Epoch 30, MSE Loss: 0.182279\n",
      "Epoch 40, MSE Loss: 0.177355\n",
      "Epoch 50, MSE Loss: 0.173387\n",
      "Epoch 60, MSE Loss: 0.169932\n",
      "Epoch 70, MSE Loss: 0.166707\n",
      "Epoch 80, MSE Loss: 0.163628\n",
      "Epoch 90, MSE Loss: 0.160698\n",
      "Epoch 100, MSE Loss: 0.157845\n",
      "Epoch 110, MSE Loss: 0.155143\n",
      "Epoch 120, MSE Loss: 0.152457\n",
      "Epoch 130, MSE Loss: 0.149829\n",
      "Epoch 140, MSE Loss: 0.147222\n",
      "Epoch 150, MSE Loss: 0.144727\n",
      "Epoch 160, MSE Loss: 0.142324\n",
      "Epoch 170, MSE Loss: 0.140030\n",
      "Epoch 180, MSE Loss: 0.137806\n",
      "Epoch 190, MSE Loss: 0.135640\n",
      "Epoch 200, MSE Loss: 0.133527\n",
      "Epoch 210, MSE Loss: 0.131442\n",
      "Epoch 220, MSE Loss: 0.129391\n",
      "Epoch 230, MSE Loss: 0.127383\n",
      "Epoch 240, MSE Loss: 0.125425\n",
      "Epoch 250, MSE Loss: 0.123525\n",
      "Epoch 260, MSE Loss: 0.121655\n",
      "Epoch 270, MSE Loss: 0.119808\n",
      "Epoch 280, MSE Loss: 0.117985\n",
      "Epoch 290, MSE Loss: 0.116148\n",
      "Epoch 300, MSE Loss: 0.114287\n",
      "Epoch 310, MSE Loss: 0.112469\n",
      "Epoch 320, MSE Loss: 0.110707\n",
      "Epoch 330, MSE Loss: 0.108992\n",
      "Epoch 340, MSE Loss: 0.107358\n",
      "Epoch 350, MSE Loss: 0.105783\n",
      "Epoch 360, MSE Loss: 0.104238\n",
      "Epoch 370, MSE Loss: 0.102743\n",
      "Epoch 380, MSE Loss: 0.101311\n",
      "Epoch 390, MSE Loss: 0.099934\n",
      "Epoch 400, MSE Loss: 0.098606\n",
      "Epoch 410, MSE Loss: 0.097321\n",
      "Epoch 420, MSE Loss: 0.096076\n",
      "Epoch 430, MSE Loss: 0.094860\n",
      "Epoch 440, MSE Loss: 0.093673\n",
      "Epoch 450, MSE Loss: 0.092516\n",
      "Epoch 460, MSE Loss: 0.091383\n",
      "Epoch 470, MSE Loss: 0.090274\n",
      "Epoch 480, MSE Loss: 0.089191\n",
      "Epoch 490, MSE Loss: 0.088137\n",
      "Epoch 500, MSE Loss: 0.087113\n",
      "Epoch 510, MSE Loss: 0.086114\n",
      "Epoch 520, MSE Loss: 0.085138\n",
      "Epoch 530, MSE Loss: 0.084189\n",
      "Epoch 540, MSE Loss: 0.083261\n",
      "Epoch 550, MSE Loss: 0.082358\n",
      "Epoch 560, MSE Loss: 0.081474\n",
      "Epoch 570, MSE Loss: 0.080607\n",
      "Epoch 580, MSE Loss: 0.079759\n",
      "Epoch 590, MSE Loss: 0.078921\n",
      "Epoch 600, MSE Loss: 0.078105\n",
      "Epoch 610, MSE Loss: 0.077307\n",
      "Epoch 620, MSE Loss: 0.076526\n",
      "Epoch 630, MSE Loss: 0.075763\n",
      "Epoch 640, MSE Loss: 0.075015\n",
      "Epoch 650, MSE Loss: 0.074284\n",
      "Epoch 660, MSE Loss: 0.073572\n",
      "Epoch 670, MSE Loss: 0.072874\n",
      "Epoch 680, MSE Loss: 0.072191\n",
      "Epoch 690, MSE Loss: 0.071524\n",
      "Epoch 700, MSE Loss: 0.070868\n",
      "Epoch 710, MSE Loss: 0.070224\n",
      "Epoch 720, MSE Loss: 0.069593\n",
      "Epoch 730, MSE Loss: 0.068978\n",
      "Epoch 740, MSE Loss: 0.068375\n",
      "Epoch 750, MSE Loss: 0.067784\n",
      "Epoch 760, MSE Loss: 0.067207\n",
      "Epoch 770, MSE Loss: 0.066642\n",
      "Epoch 780, MSE Loss: 0.066090\n",
      "Epoch 790, MSE Loss: 0.065552\n",
      "Epoch 800, MSE Loss: 0.065022\n",
      "Epoch 810, MSE Loss: 0.064501\n",
      "Epoch 820, MSE Loss: 0.063993\n",
      "Epoch 830, MSE Loss: 0.063495\n",
      "Epoch 840, MSE Loss: 0.063007\n",
      "Epoch 850, MSE Loss: 0.062530\n",
      "Epoch 860, MSE Loss: 0.062062\n",
      "Epoch 870, MSE Loss: 0.061604\n",
      "Epoch 880, MSE Loss: 0.061153\n",
      "Epoch 890, MSE Loss: 0.060710\n",
      "Epoch 900, MSE Loss: 0.060276\n",
      "Epoch 910, MSE Loss: 0.059849\n",
      "Epoch 920, MSE Loss: 0.059432\n",
      "Epoch 930, MSE Loss: 0.059023\n",
      "Epoch 940, MSE Loss: 0.058620\n",
      "Epoch 950, MSE Loss: 0.058225\n",
      "Epoch 960, MSE Loss: 0.057837\n",
      "Epoch 970, MSE Loss: 0.057456\n",
      "Epoch 980, MSE Loss: 0.057082\n",
      "Epoch 990, MSE Loss: 0.056715\n",
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(200, 2)\n",
    "y = ((X[:, 0]**2 + X[:, 1]**2) > 1.0).astype(int).reshape(-1, 1)\n",
    "V = np.random.randn(input_size, hidden_size) * np.sqrt(1/input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "W = np.random.randn(hidden_size, 1) * np.sqrt(1/hidden_size)\n",
    "b2 = np.zeros((1, 1))\n",
    "fit(X,V,W,b1, b2, y, epochs=1000)\n",
    "\n",
    "y_pred = predict(X,V,W,b1, b2,y)\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ac811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
